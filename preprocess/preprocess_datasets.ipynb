{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882b7954-a14e-4b28-a921-4eddc8821da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest preprocessing\n",
    "import cv2, os, glob, h5py, yaml, math\n",
    "import pathlib\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from data_prep_utils import octSpectralisReader as osr\n",
    "from data_prep_utils.preprocessData import preprocessData\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from astropy.convolution import convolve\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.io import savemat\n",
    "from data_prep_utils.misc import build_mask, sp_noise\n",
    "from data_prep_utils.retinaFlatten import retinaFlatten\n",
    "from data_prep_utils.retinaDetect import retinaDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1ca1ee-22e9-4e1b-b2d8-a9b7377bb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"preprocess_config_JH.yaml\"\n",
    "\n",
    "with open(config_path) as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "112bac19-ad55-4b45-b22b-4cd6f508bf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class processData:\n",
    "    def __init__(self, config):\n",
    "        self.layers = config['general']['layers']\n",
    "        self.bscans = config['general']['bscans']\n",
    "        self.width = config['general']['width']\n",
    "        self.sliding_window = config['general']['sliding_window']\n",
    "        self.height = config['general']['height']\n",
    "        self.stride = config['general']['stride']\n",
    "        self.top_offset = config['general']['top_offset']\n",
    "        self.split_ratio = config['general']['split_ratio']\n",
    "        self.seed = config['general']['seed']\n",
    "        self.getPatches = config['general']['getPatches']\n",
    "        self.crop = config['general']['crop']\n",
    "        self.constructSyntheticMap = config['general']['constructSyntheticMap']\n",
    "        \n",
    "        self.image_path = config['filepaths']['image_path']\n",
    "        self.label_path = config['filepaths']['label_path']\n",
    "        self.processed_path = config['filepaths']['processed_path']\n",
    "        self.group = config['filepaths']['group']\n",
    "        self.filename = config['filepaths']['filename']\n",
    "        self.save_filename = config['filepaths']['save_filename']\n",
    "        \n",
    "        self.data_list = config['general']['data_list']\n",
    "        \n",
    "        self.datasets = {\"training\":None, \"val\":None, \"test\":None}\n",
    "        self.postprocessed_data = None\n",
    "        \n",
    "        self.algorithm = config['algorithm']\n",
    "        \n",
    "    def save_data(self, df, mode):\n",
    "\n",
    "        df.to_csv('{}/{}/{}_reconstruct_data.csv'.format(self.processed_path,self.save_filename, mode))\n",
    "\n",
    "        f= open(\"{}/{}/{}_dataset.txt\".format(self.processed_path, self.save_filename, mode),\"w+\")\n",
    "        for i in range(len(self.datasets[mode])):\n",
    "            f.write(\"{}\\n\".format(self.datasets[mode][i]))\n",
    "        f.close() \n",
    "\n",
    "        with h5py.File(os.path.join(self.processed_path,self.save_filename,'{}_intermediate.hdf5'.format(mode)), 'w') as hf:\n",
    "            for datatype in self.data_list:\n",
    "                hf.create_dataset(datatype, data=np.array(self.postprocessed_data[datatype]))\n",
    "        hf.close()\n",
    "        \n",
    "    def prepare_dataset(self):\n",
    "        \"\"\" split into list of cases and training dataset\n",
    "        \"\"\"\n",
    "        if self.filename == 'JH' or self.filename == 'JH_MS':\n",
    "            # load all the data in the folder and sort by name \n",
    "            list_of_cases= [file for root,dirs,files in os.walk(self.image_path) for file in files if file.startswith(self.group)]\n",
    "            \n",
    "        elif self.filename == 'MIAMI_HC':\n",
    "            list_of_cases= glob.glob(os.path.join(self.image_path,self.filename,'*.mat')) \n",
    "            \n",
    "        elif self.filename == \"MIAMI_DME\":\n",
    "            list_of_cases= glob.glob(os.path.join(self.image_path,self.filename,'Patient*.mat')) \n",
    "            \n",
    "        training_dataset, self.datasets[\"test\"] = train_test_split(list_of_cases,test_size=self.split_ratio, random_state=self.seed)\n",
    "        self.datasets[\"training\"], self.datasets[\"val\"] = train_test_split(training_dataset,test_size=self.split_ratio, random_state=self.seed)\n",
    "\n",
    "    def get_sliding_window(self, data_store, patient_name): # one patient\n",
    "\n",
    "        '''\n",
    "        get sliding window with ROI (overlapping functionality can be enabled)\n",
    "        '''\n",
    "        \n",
    "        indices = [*range(0,self.width-self.sliding_window,self.stride)]\n",
    "        post = {k:np.zeros((self.bscans, len(indices), self.height, self.sliding_window)) for k in self.data_list}\n",
    "        post['lmap'] =  np.zeros((self.bscans, len(indices), self.layers+1, self.height,self.sliding_window))\n",
    "        \n",
    "        all_data = []\n",
    "        for scan in range(self.bscans):\n",
    "\n",
    "            one_data= []\n",
    "            for idx,z in enumerate(indices):\n",
    "                \n",
    "                if self.crop == True:\n",
    "                    self.window_slide = data_store['rmask'][:,z:z+self.sliding_window, scan]  \n",
    "                    positions = np.nonzero(self.window_slide)\n",
    "                    top = positions[0].min() - self.top_offset # get top position - top_offset => CHANGED THIS\n",
    "                    bottom = top+self.height\n",
    "                else:\n",
    "                    top = 0\n",
    "                    bottom = self.height\n",
    "                left = z\n",
    "                right = z+self.sliding_window\n",
    "                for datatype in self.data_list:\n",
    "                    if datatype in data_store.keys():\n",
    "                        if datatype == 'lmap':\n",
    "                            post[datatype][scan, idx] = data_store[datatype][:,top:bottom, left:right, scan]\n",
    "                        else:\n",
    "                            post[datatype][scan, idx] = data_store[datatype][top:bottom, left:right, scan]\n",
    "                \n",
    "                one_data.append([patient_name, scan, top, bottom, left, right])\n",
    "                \n",
    "            all_data.append(one_data)\n",
    "\n",
    "        return post, all_data\n",
    "    \n",
    "    \n",
    "    def process_vol_JH(self, file, annotations):\n",
    "        '''code taken from: https://github.com/steventan0110/OCT_preprocess. Added things like positional map and synthetic map\n",
    "        '''\n",
    "        # read oct files\n",
    "        [header, BScanHeader, slo, BScans] = osr.octSpectralisReader(file)\n",
    "        header['angle'] = 0\n",
    "\n",
    "        #initialize options:\n",
    "\n",
    "        preproc_params = self.algorithm['preproc_params']\n",
    "        preproc_params['retinadetector_type'] = self.algorithm['types']\n",
    "\n",
    "        pd = preprocessData(BScans, header, preproc_params, config['algorithm']['probs'], self.algorithm['scanner_type'], annotations, self.data_list)\n",
    "        pd.preprocess()\n",
    "        \n",
    "        return pd.data_store\n",
    "    \n",
    "    \n",
    "    def get_wmap(self, data_store):\n",
    "        lmap_max = np.argmax(data_store['lmap'], axis=0)\n",
    "        lmap_shifted = np.ones(lmap_max.shape)*8\n",
    "        lmap_shifted[1:] = lmap_max[:-1]\n",
    "        lmap_diff = (lmap_max - lmap_shifted)\n",
    "        lmap_diff[lmap_diff==-8] = 1\n",
    "        combined = 5*(lmap_max<8)+10*lmap_diff\n",
    "        combined[combined==10] = 15\n",
    "        wmap = combined+1\n",
    "        data_store['wmap'] = wmap  \n",
    "        return data_store\n",
    "    \n",
    "    def process_one_file(self, dataset):\n",
    "        # get patient name\n",
    "        if self.filename == 'JH' or self.filename == 'JH_MS':\n",
    "            patient_name=os.path.splitext(dataset)[0]\n",
    "            # get image files for each patient\n",
    "            label_path=os.path.join(self.label_path, self.filename, patient_name+'_label.mat')\n",
    "            image_path = os.path.join(self.image_path, self.filename, dataset)\n",
    "            # load corresponding label\n",
    "            mat = scipy.io.loadmat(label_path)\n",
    "            annotations=mat['bd_pts'] # 1024*49*9 (all the segmentations for 1 patient)\n",
    "            data_store = self.process_vol_JH(image_path, annotations)\n",
    "            data_store = self.get_wmap(data_store)\n",
    "            \n",
    "            new_lmap = np.zeros((10,data_store['lmap'].shape[1], \\\n",
    "                                 data_store['lmap'].shape[2], \\\n",
    "                                data_store['lmap'].shape[3]))\n",
    "            for num in range(data_store['lmap'].shape[3]):\n",
    "                bkgd = data_store['lmap'][8,:,:,num]\n",
    "                num_labels, labels_im = cv2.connectedComponents(np.uint8(bkgd))\n",
    "                temp = np.concatenate((np.expand_dims((labels_im==1).astype(int), axis=0), data_store['lmap'][:,:,:,num]))\n",
    "                temp[9] = (labels_im==2).astype(int)\n",
    "                new_lmap[:,:,:,num] = temp\n",
    "                \n",
    "            data_store['lmap'] = new_lmap\n",
    "        \n",
    "        return data_store, patient_name\n",
    "     \n",
    "    def create_dataset(self, mode):\n",
    "        '''\n",
    "        by patient\n",
    "        Outputs:\n",
    "        img_vol: flattened retina mask, from 0-1, shape: (496, 1024, 49)\n",
    "        positional_map: flattened relative position map from top to bottom layer, from 0-1, shape: (496, 1024, 49)\n",
    "        layer_map: flattened ground truth labels, 9 channels (8 layers + background), shape: (9, 496, 1024, 49)\n",
    "        image_with_noise: synthetic images with noise, shape: (496, 1024, 49) \n",
    "        Bscans: original OCT images \n",
    "        '''\n",
    "\n",
    "        postprocessed_data = {k:[] for k in self.data_list}\n",
    "        postprocessed_data['patient'] = []\n",
    "        postprocessed_data['meta_data'] = []\n",
    "        \n",
    "        if self.filename in [\"JH\", \"JH_MS\", \"MIAMI_HC\", \"MIAMI_DME\"]:\n",
    "            for count,dataset in enumerate(self.datasets[mode]):\n",
    "\n",
    "                data_store, all_data = self.process_one_file(dataset)\n",
    "                \n",
    "                if self.getPatches == True:\n",
    "                    data_store, all_data = self.get_sliding_window(data_store, all_data)\n",
    "\n",
    "                for datatype in self.data_list:\n",
    "                    if datatype in data_store.keys():\n",
    "                        postprocessed_data[datatype].append(data_store[datatype])\n",
    "                postprocessed_data['patient'].append(dataset)\n",
    "                if self.getPatches == True: postprocessed_data['meta_data'].append(all_data)\n",
    "\n",
    "                    # construct layered image for each scan\n",
    "                    # save the order at which the files are created on a text file\n",
    "                f = open((\"{}/{}/{}_order_files.txt\".format(self.processed_path, self.save_filename, mode)), \"a\")\n",
    "                f.write(dataset+\"\\n\")\n",
    "                f.close()\n",
    "        \n",
    "        return postprocessed_data\n",
    "    \n",
    "        \n",
    "    def make_dataset(self, mode, save):\n",
    "        '''\n",
    "        mode refers to training, test or val\n",
    "        '''\n",
    "        pathlib.Path(os.path.join(self.processed_path,self.save_filename)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        postprocessed_data = self.create_dataset(mode)\n",
    "        self.postprocessed_data = postprocessed_data\n",
    "        # save data information and files\n",
    "        flat_list = [item for sublist in process_data.postprocessed_data['meta_data'] for innerlist in sublist for item in innerlist]\n",
    "        if save == True:\n",
    "\n",
    "            df = pd.DataFrame(flat_list, columns = ['patient_name', 'slice_number','top','bottom','left','right']) \n",
    "            self.save_data(df, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "604420de-136c-45ce-88c5-a781ee68efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_data = processData(config)\n",
    "\n",
    "process_data.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6aa3b6f1-64ec-4f59-8b1a-6e228341ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! 227 outlier points\n",
      "Preparing S map\n",
      "done! 1975 outlier points\n",
      "Warning: poor fit of retina boundaries detected (1975 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 2229 outlier points\n",
      "Warning: poor fit of retina boundaries detected (2229 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 1273 outlier points\n",
      "Warning: poor fit of retina boundaries detected (1273 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 2426 outlier points\n",
      "Warning: poor fit of retina boundaries detected (2426 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 191 outlier points\n",
      "Preparing S map\n",
      "done! 519 outlier points\n",
      "Warning: poor fit of retina boundaries detected (519 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 142 outlier points\n",
      "Preparing S map\n",
      "done! 111 outlier points\n",
      "Preparing S map\n",
      "done! 733 outlier points\n",
      "Warning: poor fit of retina boundaries detected (733 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 2602 outlier points\n",
      "Warning: poor fit of retina boundaries detected (2602 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 417 outlier points\n",
      "Preparing S map\n",
      "done! 1513 outlier points\n",
      "Warning: poor fit of retina boundaries detected (1513 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 740 outlier points\n",
      "Warning: poor fit of retina boundaries detected (740 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data.make_dataset(\"training\", True)\n",
    "process_data.make_dataset(\"val\", True)\n",
    "process_data.make_dataset(\"test\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e000e-847d-4f77-8e23-f5048ee83c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

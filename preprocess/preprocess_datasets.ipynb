{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "882b7954-a14e-4b28-a921-4eddc8821da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest preprocessing\n",
    "import cv2, os, glob, h5py, yaml, math\n",
    "import pathlib\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from dataUtils import octSpectralisReader as osr\n",
    "from dataUtils.preprocessData import preprocessData\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from astropy.convolution import convolve\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.io import savemat\n",
    "from dataUtils.misc import build_mask, sp_noise\n",
    "from dataUtils.retinaFlatten import retinaFlatten\n",
    "from dataUtils.retinaDetect import retinaDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1ca1ee-22e9-4e1b-b2d8-a9b7377bb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"preprocess_config_JH.yaml\"\n",
    "\n",
    "with open(config_path) as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "112bac19-ad55-4b45-b22b-4cd6f508bf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class processData:\n",
    "    def __init__(self, config):\n",
    "        self.layers = config['general']['layers']\n",
    "        self.bscans = config['general']['bscans']\n",
    "        self.width = config['general']['width']\n",
    "        self.sliding_window = config['general']['sliding_window']\n",
    "        self.height = config['general']['height']\n",
    "        self.stride = config['general']['stride']\n",
    "        self.top_offset = config['general']['top_offset']\n",
    "        self.split_ratio = config['general']['split_ratio']\n",
    "        self.seed = config['general']['seed']\n",
    "        self.getPatches = config['general']['getPatches']\n",
    "        self.crop = config['general']['crop']\n",
    "        self.constructSyntheticMap = config['general']['constructSyntheticMap']\n",
    "        \n",
    "        self.image_path = config['filepaths']['image_path']\n",
    "        self.label_path = config['filepaths']['label_path']\n",
    "        self.processed_path = config['filepaths']['processed_path']\n",
    "        self.group = config['filepaths']['group']\n",
    "        self.filename = config['filepaths']['filename']\n",
    "        self.save_filename = config['filepaths']['save_filename']\n",
    "        \n",
    "        self.data_list = config['general']['data_list']\n",
    "        \n",
    "        self.datasets = {\"training\":None, \"val\":None, \"test\":None}\n",
    "        self.postprocessed_data = None\n",
    "        \n",
    "        self.algorithm = config['algorithm']\n",
    "        \n",
    "    def save_data(self, df, mode):\n",
    "\n",
    "        df.to_csv('{}/{}/{}_reconstruct_data.csv'.format(self.processed_path,self.save_filename, mode))\n",
    "\n",
    "        f= open(\"{}/{}/{}_dataset.txt\".format(self.processed_path, self.save_filename, mode),\"w+\")\n",
    "        for i in range(len(self.datasets[mode])):\n",
    "            f.write(\"{}\\n\".format(self.datasets[mode][i]))\n",
    "        f.close() \n",
    "\n",
    "        with h5py.File(os.path.join(self.processed_path,self.save_filename,'{}_intermediate.hdf5'.format(mode)), 'w') as hf:\n",
    "            for datatype in self.data_list:\n",
    "                hf.create_dataset(datatype, data=np.array(self.postprocessed_data[datatype]))\n",
    "        hf.close()\n",
    "        \n",
    "    def prepare_dataset(self):\n",
    "        \"\"\" split into list of cases and training dataset\n",
    "        \"\"\"\n",
    "        if self.filename == 'JH' or self.filename == 'JH_MS':\n",
    "            # load all the data in the folder and sort by name \n",
    "            list_of_cases= [file for root,dirs,files in os.walk(self.image_path) for file in files if file.startswith(self.group)]\n",
    "            \n",
    "        elif self.filename == 'MIAMI_HC':\n",
    "            list_of_cases= glob.glob(os.path.join(self.image_path,self.filename,'*.mat')) \n",
    "            \n",
    "        elif self.filename == \"MIAMI_DME\":\n",
    "            list_of_cases= glob.glob(os.path.join(self.image_path,self.filename,'Patient*.mat')) \n",
    "            \n",
    "        training_dataset, self.datasets[\"test\"] = train_test_split(list_of_cases,test_size=self.split_ratio, random_state=self.seed)\n",
    "        self.datasets[\"training\"], self.datasets[\"val\"] = train_test_split(training_dataset,test_size=self.split_ratio, random_state=self.seed)\n",
    "\n",
    "    def get_sliding_window(self, data_store, patient_name): # one patient\n",
    "\n",
    "        '''\n",
    "        get sliding window with ROI (overlapping functionality can be enabled)\n",
    "        '''\n",
    "        \n",
    "        indices = [*range(0,self.width-self.sliding_window,self.stride)]\n",
    "        post = {k:np.zeros((self.bscans, len(indices), self.height, self.sliding_window)) for k in self.data_list}\n",
    "        post['lmap'] =  np.zeros((self.bscans, len(indices), self.layers, self.height,self.sliding_window))\n",
    "        \n",
    "        all_data = []\n",
    "        for scan in range(self.bscans):\n",
    "\n",
    "            one_data= []\n",
    "            for idx,z in enumerate(indices):\n",
    "                \n",
    "                if self.crop == True:\n",
    "                    self.window_slide = data_store['rmask'][:,z:z+self.sliding_window, scan]  \n",
    "                    positions = np.nonzero(self.window_slide)\n",
    "                    top = positions[0].min() - self.top_offset # get top position - top_offset => CHANGED THIS\n",
    "                    bottom = top+self.height\n",
    "                else:\n",
    "                    top = 0\n",
    "                    bottom = self.height\n",
    "                left = z\n",
    "                right = z+self.sliding_window\n",
    "                for datatype in self.data_list:\n",
    "                    if datatype in data_store.keys():\n",
    "                        if datatype == 'lmap':\n",
    "                            post[datatype][scan, idx] = data_store[datatype][:,top:bottom, left:right, scan]\n",
    "                        else:\n",
    "                            post[datatype][scan, idx] = data_store[datatype][top:bottom, left:right, scan]\n",
    "                \n",
    "                one_data.append([patient_name, scan, top, bottom, left, right])\n",
    "                \n",
    "            all_data.append(one_data)\n",
    "\n",
    "        return post, all_data\n",
    "    \n",
    "    \n",
    "    def process_vol_JH(self, file, annotations):\n",
    "        '''code taken from: https://github.com/steventan0110/OCT_preprocess. Added things like positional map and synthetic map\n",
    "        '''\n",
    "        # read oct files\n",
    "        [header, BScanHeader, slo, BScans] = osr.octSpectralisReader(file)\n",
    "        header['angle'] = 0\n",
    "\n",
    "        #initialize options:\n",
    "\n",
    "        preproc_params = self.algorithm['preproc_params']\n",
    "        preproc_params['retinadetector_type'] = self.algorithm['types']\n",
    "\n",
    "        pd = preprocessData(BScans, header, preproc_params, config['algorithm']['probs'], self.algorithm['scanner_type'], annotations, self.data_list)\n",
    "        pd.preprocess()\n",
    "        \n",
    "        return pd.data_store\n",
    "    \n",
    "    def process_vol_Miami(self, images, annotations):\n",
    "        \n",
    "        header = {}\n",
    "        header['SizeX'] = self.width\n",
    "        header['NumBScans'] = images.shape[2]\n",
    "        header['SizeZ'] = images.shape[0]\n",
    "        header['Distance'] = 0.13 # I believe this is the slice separation?\n",
    "        header['ScaleZ'] = 0.0039\n",
    "        header['ScaleX'] = 0.012 # I believe this is lateral resolution?\n",
    "        header['angle'] = 0 \n",
    "        preproc_params = self.algorithm['preproc_params']\n",
    "        preproc_params['retinadetector_type'] = self.algorithm['types']\n",
    "        images = images/255\n",
    "        \n",
    "        pd = preprocessData(images, header, preproc_params, config['algorithm']['probs'], self.algorithm['scanner_type'], annotations, self.data_list)\n",
    "        pd.preprocess()\n",
    "        \n",
    "        return pd.data_store\n",
    "    def process_vol_Miami_DME(self, images, annotations):\n",
    "        # obtained from paper\n",
    "        \n",
    "        header = {}\n",
    "        header['SizeX'] = self.width\n",
    "        header['NumBScans'] = images.shape[2]\n",
    "        header['SizeZ'] = images.shape[0]\n",
    "        header['Distance'] = 0.13 # I believe this is the slice separation?\n",
    "        header['ScaleZ'] = 0.00387\n",
    "        header['ScaleX'] = 0.01111 # I believe this is lateral resolution?\n",
    "        header['angle'] = 0 \n",
    "        preproc_params = self.algorithm['preproc_params']\n",
    "        preproc_params['retinadetector_type'] = self.algorithm['types']\n",
    "        images = images/255\n",
    "        \n",
    "        pd = preprocessData(images, header, preproc_params, config['algorithm']['probs'], self.algorithm['scanner_type'], annotations, self.data_list)\n",
    "        pd.preprocess()\n",
    "        \n",
    "        return pd.data_store\n",
    "    \n",
    "    def get_wmap(self, data_store):\n",
    "        lmap_max = np.argmax(data_store['lmap'], axis=0)\n",
    "        retinal_mask = data_store['rmask']\n",
    "        lmap_shifted = np.ones(lmap_max.shape)*8\n",
    "        lmap_shifted[1:] = lmap_max[:-1]\n",
    "        wmap = 1+5*(retinal_mask).astype(int) + 10*((lmap_max - lmap_shifted)!=0).astype(int)\n",
    "        data_store['wmap'] = wmap    \n",
    "        \n",
    "        return data_store\n",
    "    \n",
    "    def process_one_file(self, dataset):\n",
    "        # get patient name\n",
    "        if self.filename == 'JH' or self.filename == 'JH_MS':\n",
    "            patient_name=os.path.splitext(dataset)[0]\n",
    "            # get image files for each patient\n",
    "            label_path=os.path.join(self.label_path, self.filename, patient_name+'_label.mat')\n",
    "            image_path = os.path.join(self.image_path, self.filename, dataset)\n",
    "            # load corresponding label\n",
    "            mat = scipy.io.loadmat(label_path)\n",
    "            annotations=mat['bd_pts'] # 1024*49*9 (all the segmentations for 1 patient)\n",
    "            data_store = self.process_vol_JH(image_path, annotations)\n",
    "            data_store = self.get_wmap(data_store)\n",
    "            \n",
    "        elif self.filename == 'MIAMI_HC':\n",
    "            patient_name=os.path.splitext(os.path.basename(dataset))[0]\n",
    "            # load corresponding label\n",
    "            data_path=os.path.join(self.image_path,self.filename,patient_name+'.mat')\n",
    "            mat = scipy.io.loadmat(data_path)\n",
    "            annotations=mat['Observer1'] # 1024*49*9 (all the segmentations for 1 patient)\n",
    "            images =mat['volumedata']\n",
    "            data_store = self.process_vol_Miami(images, annotations)\n",
    "            data_store = self.get_wmap(data_store)\n",
    "            \n",
    "        elif self.filename == 'MIAMI_DME':\n",
    "            patient_name=os.path.splitext(os.path.basename(dataset))[0]\n",
    "            # load corresponding label\n",
    "            data_path=os.path.join(self.image_path,self.filename,patient_name+'.mat')\n",
    "            mat = scipy.io.loadmat(data_path)\n",
    "            annotations=np.transpose(mat['annotations'],(1,2,0)) # 1024*49*9 (all the segmentations for 1 patient)\n",
    "            images =mat['images']\n",
    "            data_store = self.process_vol_Miami_DME(images,annotations)\n",
    "            data_store = self.get_wmap(data_store)\n",
    "        \n",
    "        return data_store, patient_name\n",
    "    \n",
    "\n",
    "        \n",
    "    def create_dataset(self, mode):\n",
    "        '''\n",
    "        by patient\n",
    "        Outputs:\n",
    "        img_vol: flattened retina mask, from 0-1, shape: (496, 1024, 49)\n",
    "        positional_map: flattened relative position map from top to bottom layer, from 0-1, shape: (496, 1024, 49)\n",
    "        layer_map: flattened ground truth labels, 9 channels (8 layers + background), shape: (9, 496, 1024, 49)\n",
    "        image_with_noise: synthetic images with noise, shape: (496, 1024, 49) \n",
    "        Bscans: original OCT images \n",
    "        '''\n",
    "\n",
    "        postprocessed_data = {k:[] for k in self.data_list}\n",
    "        postprocessed_data['patient'] = []\n",
    "        postprocessed_data['meta_data'] = []\n",
    "        \n",
    "        if self.filename in [\"JH\", \"JH_MS\", \"MIAMI_HC\", \"MIAMI_DME\"]:\n",
    "            for count,dataset in enumerate(self.datasets[mode]):\n",
    "\n",
    "                data_store, all_data = self.process_one_file(dataset)\n",
    "                \n",
    "                if self.getPatches == True:\n",
    "                    data_store, all_data = self.get_sliding_window(data_store, all_data)\n",
    "\n",
    "                for datatype in self.data_list:\n",
    "                    if datatype in data_store.keys():\n",
    "                        postprocessed_data[datatype].append(data_store[datatype])\n",
    "                postprocessed_data['patient'].append(dataset)\n",
    "                if self.getPatches == True: postprocessed_data['meta_data'].append(all_data)\n",
    "\n",
    "                    # construct layered image for each scan\n",
    "                    # save the order at which the files are created on a text file\n",
    "                f = open((\"{}/{}/{}_order_files.txt\".format(self.processed_path, self.save_filename, mode)), \"a\")\n",
    "                f.write(dataset+\"\\n\")\n",
    "                f.close()\n",
    "        \n",
    "        return postprocessed_data\n",
    "    \n",
    "        \n",
    "    def make_dataset(self, mode, save):\n",
    "        '''\n",
    "        mode refers to training, test or val\n",
    "        '''\n",
    "        pathlib.Path(os.path.join(self.processed_path,self.save_filename)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        postprocessed_data = self.create_dataset(mode)\n",
    "        self.postprocessed_data = postprocessed_data\n",
    "        # save data information and files\n",
    "        flat_list = [item for sublist in process_data.postprocessed_data['meta_data'] for innerlist in sublist for item in innerlist]\n",
    "        if save == True:\n",
    "\n",
    "            df = pd.DataFrame(flat_list, columns = ['patient_name', 'slice_number','top','bottom','left','right']) \n",
    "            self.save_data(df, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "604420de-136c-45ce-88c5-a781ee68efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_data = processData(config)\n",
    "\n",
    "process_data.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a20460-f286-4cf9-88db-c8c523a15923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! 227 outlier points\n",
      "Preparing S map\n",
      "done! 1975 outlier points\n",
      "Warning: poor fit of retina boundaries detected (1975 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 2229 outlier points\n",
      "Warning: poor fit of retina boundaries detected (2229 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 1273 outlier points\n",
      "Warning: poor fit of retina boundaries detected (1273 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 2426 outlier points\n",
      "Warning: poor fit of retina boundaries detected (2426 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 191 outlier points\n",
      "Preparing S map\n",
      "done! 519 outlier points\n",
      "Warning: poor fit of retina boundaries detected (519 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 142 outlier points\n",
      "Preparing S map\n",
      "done! 111 outlier points\n",
      "Preparing S map\n",
      "done! 733 outlier points\n",
      "Warning: poor fit of retina boundaries detected (733 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 2602 outlier points\n",
      "Warning: poor fit of retina boundaries detected (2602 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 417 outlier points\n",
      "Preparing S map\n",
      "done! 1513 outlier points\n",
      "Warning: poor fit of retina boundaries detected (1513 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n",
      "done! 740 outlier points\n",
      "Warning: poor fit of retina boundaries detected (740 outlier points). Check for artifacts in the data.\n",
      "\n",
      "Preparing S map\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data.make_dataset(\"training\", True)\n",
    "process_data.make_dataset(\"val\", True)\n",
    "process_data.make_dataset(\"test\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed106f9-b0a0-4280-8e0e-05311a6e01c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
